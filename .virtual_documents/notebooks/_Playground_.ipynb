from catboost.datasets import titanic
from catboost import CatBoostClassifier, Pool, metrics, cv
from pathlib import Path
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
import numpy as np
import os
while "freqtrade" not in os.listdir():
    os.chdir("..")

train_df, test_df = titanic()
null_value_stats = train_df.isnull().sum(axis=0)
null_value_stats
train_df.fillna(-999, inplace=True)
test_df.fillna(-999, inplace=True)
X = train_df.drop('Survived', axis=1)
y = train_df.Survived
print(X.dtypes)
categorical_features_indices = np.where(X.dtypes != float)[0]

X_train, X_validation, y_train, y_validation = train_test_split(X, y, train_size=0.75, random_state=42)
X_test = test_df

model = CatBoostClassifier(
    iterations=5000,
    custom_loss=[metrics.Accuracy()],
    random_seed=42,
    logging_level='Silent',
    task_type="GPU",
)
model.fit(
    X_train, y_train,
    cat_features=categorical_features_indices,
    eval_set=(X_validation, y_validation),
#     logging_level='Verbose',  # you can uncomment this for text output
    plot=True
)


# Use XGBoost to train Iris model
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, accuracy_score
import xgboost as xgb
import numpy as np

# Start training
iris = load_iris()
X = iris.data
y = iris.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

param = {
    'max_depth': 1,  # the maximum depth of each tree
    'eta': 0.3,  # the training step for each iteration
    'silent': 0,  # logging mode - quiet
    'objective': 'multi:softprob',  # error evaluation for multiclass training
    'num_class': 3,
    'tree_method': 'gpu_hist',
}  # the number of classes that exist in this datset

num_round = 20000 # the number of training iterations

bst = xgb.train(param, dtrain, num_round)

preds = bst.predict(dtest)
best_preds = np.asarray([np.argmax(line) for line in preds])

print("Precision = {}".format(precision_score(y_test, best_preds, average='macro')))
print("Recall = {}".format(recall_score(y_test, best_preds, average='macro')))
print("Accuracy = {}".format(accuracy_score(y_test, best_preds)))


from distributed import Client
os.environ["MODIN_ENGINE"] = "dask"
client = Client(n_workers=6)
import modin.pandas as mpd


path_data = Path.cwd().parent / "mount" / "data" / "binance" / "BTC_USDT-1m.json"
headers = ["date", "open", "high", "low", "close", "volume"]
d1 = pd.read_json(path_data)
d1.columns = headers
d1["date"] = pd.to_datetime(d1["date"], unit='ms', utc=True, infer_datetime_format=True)
d1 = d1.astype(
    dtype={'open': 'float', 'high': 'float', 'low': 'float', 'close': 'float', 'volume': 'float'}
)
d1


from freqtrade.ml.loader import clean_ohlcv_dataframe
d1 = clean_ohlcv_dataframe(d1, "1m", fill_missing=True, drop_incomplete=True)
d1


import pandas as pd
df = pd.DataFrame({
    "no": [5, 8, 1],
    "ml_1": [0, 0, 1],
    "ml_2": [1, 0.5, 0],
    "buy": [0,0,0]
})
df["argmax_ml"] = df[["ml_1", "ml_2"]].idxmax(axis=1)
df.loc[df["argmax_ml"] == "ml_1", "buy"] = 1
df = df[["ml_1", "ml_2"]]
df


# TODO: Store a list of row indexes that has NaN
# Then predict using non NaN rows
# Then concat predictions to non NaN indexes
import numpy as np
import pandas as pd

df = pd.DataFrame({
    "price1": [2, 4.1, 0.3, np.nan, 9],
    "price2": [3, 0.2, np.nan, 8, 99]
})

nan_indexes = df[df.isnull().any(axis=1)].index
    non_nan_indexes = df[~df.isnull().any(axis=1)].index
df.loc[non_nan_indexes]


preds = np.array([[4.0], [3.0], [4.0]])
preds


# Assuming orders preserved, we can safely concat preds to non nan indexes
# (Len of preds == Len non nan rows)
df_preds = pd.DataFrame(preds)
df_preds.index = non_nan_indexes
df_preds


df_with_preds = pd.concat([df, df_preds], axis=1)
df_with_preds


df_with_preds.describe()


import tensorflow as tf

mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(10)
])


loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(optimizer='adam',
              loss=loss_fn,
              metrics=['accuracy'])
model.fit(x_train, y_train, epochs=50)





tf.config.get_visible_devices()


import catboost
catboost.__version__



